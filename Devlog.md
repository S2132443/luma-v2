# Multimodal AI VTuber/Streamer Devlog

## Phase 1: Text Chat + Hybrid Memory + Web Interface

### Overview
Extend the current Luma-v2 Discord bot architecture to implement Phase 1 functionality: text chat with hybrid memory system, enhanced web interface with document upload capabilities, and configurable AI memory suggestions. This phase enhances the existing Flask-based dashboard and Discord bot to include manual memory management, AI-suggested memories using the existing DeepSeek API with support for local Ollama models, and a new web chat interface.

### Goals
- Enhance current web-based chat interface with memory management capabilities
- Implement hybrid memory system with manual and AI-suggested entries
- Maintain existing settings and logging functionality
- Support both DeepSeek API and Ollama local models
- Preserve current Flask-based frontend while adding memory features
- Add new web chat interface replacing the landing page
- Implement document upload functionality for multiple formats (PDF, Excel, TXT, JSON, CSV)
- Add configurable AI memory suggestions toggle (off by default)
- Implement message history persistence using browser storage

### Technical Architecture

#### Backend Enhancements
- **API Configuration**: Extend current settings to support both DeepSeek API and Ollama endpoints
- **Memory System**: Enhance the existing `Memory` model to support different types:
  - Manual memories (user-added)
  - AI-suggested memories (automatically generated by LLM)
- **LLM Integration**: Implement a pluggable model interface supporting both DeepSeek API and Ollama
- **Memory Suggestions**: Modify LLM responses to include memory suggestions based on conversation content

#### Database Extensions
- Extend existing `Memory` table with:
  - `source` column: 'manual' or 'ai_suggested'
  - `importance` column: score for memory relevance
  - `tags` column: JSON array for memory categorization
- Potentially add a `memory_suggestions` table to track AI-generated memory proposals

#### Frontend Enhancements (Flask Templates)
- **Memory Management Panel**: Add section to current dashboard for viewing and managing memories
- **Memory Suggestions Interface**: Display AI-suggested memories with approve/reject options
- **Enhanced Chat Interface**: Add in-memory conversation context to settings page
- **Memory Search**: Add capability to search and filter memories

### Implementation Tasks

#### 1. Model & Database Extensions
- [x] Extend `shared/models.py` Memory model with source, importance, and tags fields
- [x] Update SQLAlchemy models to support new memory attributes
- [x] Create migration scripts to add new columns to existing memory table (Database schema changes are applied automatically by SQLAlchemy)

#### 2. Model Abstraction Layer
- [x] Create `backend/llm_interface.py` for pluggable AI models
- [x] Implement DeepSeek adapter (extending current functionality)
- [x] Implement Ollama adapter
- [x] Create configuration system to switch between model providers

#### 3. Memory Management Backend
- [x] Create `backend/memory_manager.py` for enhanced memory operations
- [x] Implement memory suggestion logic in chat flow
- [x] Add API endpoints for memory search and management
- [x] Implement memory retrieval system to enhance context

#### 4. Web Dashboard Enhancements
- [x] Update `/memory` endpoint with enhanced memory management features
- [x] Add memory suggestion approval interface
- [x] Implement memory search and filtering
- [x] Add model provider selection in settings

#### 5. Discord Bot Integration
- [x] Enhance current `/chat` command to generate memory suggestions
- [x] Modify conversation context to include relevant memories
- [ ] Add new memory-related commands (optional)
- [x] Update token usage tracking to account for memory retrieval

#### 6. Configuration & Settings
- [x] Extend settings page to support Ollama endpoint configuration
- [x] Add model provider toggle (DeepSeek vs Ollama)
- [x] Implement model-specific settings (temperature, max_tokens, etc.)
- [x] Preserve backward compatibility with existing settings

#### 7. Testing & Validation
- [x] Write tests for new memory functionality
- [ ] Test both DeepSeek and Ollama integration
- [ ] Validate memory suggestion accuracy
- [ ] Performance testing with increasing memory size

#### 8. Documentation
- [x] Update README with Phase 1 features
- [x] Document new API endpoints
- [x] Add configuration examples for Ollama
- [x] Update Docker compose for potential Ollama service (optional)

### Implementation Flow

```
User Text â†’ Model Selection (DeepSeek/Ollama) â†’ Memory Retrieval â†’ Main LLM â†’
          â”œâ”€ Text Response â†’ Frontend/Discord
          â””â”€ Memory Suggestion â†’ Memory DB (if approved)
                        â–²
                        â”‚
           User manual add/approve/delete/update
```

### Database Schema Changes
```sql
-- Extend existing memories table
ALTER TABLE memories ADD COLUMN source VARCHAR(20) DEFAULT 'manual';
ALTER TABLE memories ADD COLUMN importance INTEGER DEFAULT 0;
ALTER TABLE memories ADD COLUMN tags JSON;
ALTER TABLE memories ADD COLUMN approved BOOLEAN DEFAULT TRUE;
```

### New API Endpoints
- `GET /api/memory/search?q=query` - Search memories by content
- `POST /api/memory/suggest` - Approve AI-suggested memory
- `PUT /api/memory/{id}` - Update memory importance/tags
- `GET /api/models/available` - List available models/providers

### Configuration Settings
- `model_provider`: 'deepseek' or 'ollama'
- `deepseek_api_key`: API key for DeepSeek
- `ollama_endpoint`: URL for Ollama service (e.g., 'http://ollama:11434')
- `ollama_model`: Name of Ollama model to use
- `memory_context_limit`: Max memories to include in context
- `memory_suggestion_threshold`: Minimum importance score for suggestions

### Docker Compose Updates
```yaml
# Optional Ollama service (for local models)
ollama:
  image: ollama/ollama:latest
  ports:
    - "11434:11434"
  volumes:
    - ollama_data:/root/.ollama
  # Uncomment to run on GPU
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]

webapp:
  # Add environment for model selection
  environment:
    - MODEL_PROVIDER=deepseek  # or ollama
    - OLLAMA_ENDPOINT=http://ollama:11434  # if using ollama
```

### Success Criteria
- [x] Users can view, add, edit, and delete memories via web interface
- [x] AI suggests relevant memories during conversations
- [x] Both DeepSeek API and Ollama models work seamlessly
- [x] Memory retrieval improves conversation context
- [x] Existing functionality remains intact
- [x] Performance remains acceptable with growing memory size

### Timeline
- **Week 1**: Database extensions and model abstraction
- **Week 2**: Memory management backend and APIs
- **Week 3**: Frontend enhancements and Discord bot updates
- **Week 4**: Testing, documentation, and deployment

### Next Steps
After Phase 1 completion, prepare for Phase 2 by:
- Setting up ASR/TTS service infrastructure
- Planning WebSocket integration for real-time chat
- Preparing frontend for real-time updates

## Additional Features Beyond Original Scope

The following features were implemented beyond the original Phase 1 plan:

### Web Chat Interface
- Replaced landing page with full-featured chat interface
- Real-time messaging with history
- Default user ID for web-based interactions
- Responsive design compatible with existing dashboard

### Document Upload System
- Support for multiple file formats (PDF, Excel, TXT, JSON, CSV)
- Automatic content extraction and processing
- Integration with memory system to store document content
- File validation and error handling

### Memory Suggestions Control
- Configurable toggle in settings (off by default)
- Backend logic to conditionally generate suggestions
- Token usage optimization when disabled

### Message Persistence
- Browser-based chat history storage using localStorage
- Automatic loading of previous conversations
- Message history limits to prevent storage bloat

### Ollama Model Detection Feature
- Dynamic model detection from Ollama service
- API endpoint to fetch available models from Ollama
- JavaScript to populate dropdown with available models
- Multiple endpoint fallbacks to handle different network configurations (host.docker.internal, direct IP, etc.)

### ChatGPT-style File Upload
- Compact "Upload" button integrated with text input area
- Improved UI alignment with consistent element heights
- Better user experience with file selection and upload feedback

### Settings Page Enhancement
- Dynamic UI that shows appropriate fields based on model provider selection
- Proper JavaScript initialization to show correct section on page load
- Correct toggle behavior between DeepSeek and Ollama settings

## Phase 1: COMPLETE âœ…

Phase 1 has been successfully implemented with all core features:
- Enhanced memory system with manual and AI-suggested entries
- Multi-model support (DeepSeek API and Ollama)
- Improved web dashboard with memory management
- Discord bot with memory suggestion capabilities
- API endpoints for memory operations
- Complete Docker configuration with optional Ollama service
- New web chat interface with document upload
- Configurable memory suggestions with toggle
- Message history persistence
- Ollama model detection and dynamic settings
- ChatGPT-style file upload interface
- Improved UI alignment and user experience

## Major Documentation & Architecture Updates

### Comprehensive README.md Overhaul

**Date:** December 1, 2025

#### 1. **Configurable LLM Provider System** âœ…
- Added comprehensive AI Models table with configurable providers for each component
- **Main Thinking LLM**: DeepSeek V3, Qwen 7B, Llama 3 (Local) vs OpenAI GPT-4o, Claude 3 (API) vs GPT-3.5-turbo, Claude Haiku (OpenRouter Free)
- **Memory Manager LLM**: Phi-3 Mini, Qwen 1.8B (Local) vs GPT-3.5 Mini, Claude Haiku (API) vs GPT-3.5-turbo, Llama 3 (OpenRouter Free)
- **Vision LLM**: Qwen-VL 7B, MiniGPT-4 (Local) vs GPT-4o Vision API, Claude Vision (API) vs LLaVA, Gemini Flash (OpenRouter Free)
- **Voice Model (TTS)**: Kokoro, DIA 1.6B (Local) vs ElevenLabs, Google TTS (API) vs Coqui TTS, Edge TTS (OpenRouter Free)
- **ASR (Speech-to-Text)**: Whisper.cpp, Faster-whisper (Local) vs OpenAI Whisper API, Deepgram (API) vs Vosk, AssemblyAI Free Tier (OpenRouter Free)
- **Internet LLM**: Llama 3, Mixtral (Local) vs Perplexity API, Phind (API) vs OpenRouter Free Models

#### 2. **API Provider Management System** âœ…
- **OpenRouter Integration**: Unified API for accessing multiple providers with free tier options
- **Rate Limit Monitoring**: Automatic tracking of API usage across all providers
- **Cost Optimization**: Smart fallback to cheaper/free models when limits are approached
- **Provider Health Checks**: Monitor API availability and response times

#### 3. **API Limit Warnings & Service Management** âœ…
- **Smart Shutdown**: Services automatically disable when approaching API limits
- **"Luma Getting Sleepy"**: Friendly notifications when Main Memory LLM limits are reached
- **"Luma Asleep"**: Clear indication when services are disabled due to exhausted limits
- **Limit Recovery**: Automatic service re-enabling when limits reset

#### 4. **New Phase 6: Internet LLM Integration** âœ…
- **Goals**: Enable AI to search the internet for current information, web browsing and information retrieval capabilities
- **Components**: Internet LLM container, Web search API integration, Research task management, Content filtering and summarization
- **Flow**: Research Request â†’ Internet LLM â†’ Web Search â†’ Information Retrieval â†’ Summarization â†’ Main LLM â†’ Response

#### 5. **New Phase 7: API Limit Monitoring & Token Management** âœ…
- **Goals**: Real-time monitoring of API token usage across all providers, automatic service shutdown when approaching limits, friendly notifications, cost optimization and budget management
- **Components**: Token usage tracking service, API limit monitoring system, Service health dashboard, Notification system, Automatic fallback management
- **Flow**: API Usage â†’ Token Tracker â†’ Limit Monitor â†’ Warning Notifications â†’ Dashboard, Service Shutdown â†’ Graceful Degradation, Cost Optimization â†’ Provider Switching

#### 6. **New Phase 8: Advanced Monitoring Dashboard** âœ…
- **Goals**: GPU usage monitoring for Ollama users, system performance metrics, combined with Phase 7's token monitoring, hardware resource tracking, performance optimization insights
- **Components**: GPU monitoring service, System metrics collection, Performance dashboard, Resource utilization tracking, Hardware health monitoring
- **Flow**: System Metrics â†’ Monitoring Service â†’ Dashboard â†’ GPU Usage â†’ Performance Insights, Memory Usage â†’ Optimization Suggestions, CPU Usage â†’ Resource Management

#### 7. **Phase Renumbering** âœ…
- Original Phase 6 (OBS + Twitch Chat Integration) â†’ **Phase 9**
- Original Phase 7 (Discord Bot) â†’ **Phase 10**
- Added **Phase 6**: Internet LLM Integration
- Added **Phase 7**: API Limit Monitoring & Token Management
- Added **Phase 8**: Advanced Monitoring Dashboard

#### 8. **Enhanced Dockerized Architecture** âœ…
Updated architecture diagram to include:
- Frontend: Chat + Avatar + Webcam + OBS + Dashboard + Monitoring
- Monitoring Service: Token Usage + GPU Stats
- API Limit Monitor with Service Health
- Internet LLM â†’ Web Search API
- Notifications ("Luma Sleepy/Asleep")
- Additional containers: TTS, Internet LLM, GPU Monitor, Token Tracker

#### 9. **Comprehensive Next Steps Update** âœ…
- Marked Phase 1 as COMPLETE with âœ…
- Added detailed descriptions for all new phases (6-10)
- Added Configuration Management section with feature toggles, model provider selection, API limit monitoring, GPU monitoring
- Added Model Provider Selection section with OpenRouter, Local Models, Cloud APIs, Automatic Fallback

#### 10. **Expanded Suggested Folder Structure** âœ…
- **Backend**: Added internet_search.py, monitoring.py, internet_llm.py, monitoring_service.py, api_limits.py, gpu_monitor.py, cost_optimizer.py, notifications.py, settings.py
- **Webapp Components**: Added InternetSearch.jsx, MonitoringDashboard.jsx, Notifications.jsx, SettingsPage.jsx
- **New Directories**: monitoring/ (GPU monitoring, token tracking, health checks), docs/ (API documentation, model configuration, deployment guides)

### Updated Docker Compose Architecture

#### 1. **Enhanced Service Structure** âœ…
- **Backend API Server**: Phase 1-10, ports 8000:8000, environment variables for all model providers
- **Web Frontend**: Phase 1-10, ports 5000:5000, connects to backend
- **Discord Bot**: Phase 10, depends on backend and db
- **Ollama Service**: Enhanced with OLLAMA_HOST and OLLAMA_ORIGINS environment variables

#### 2. **New Phase 2 Services (Voice Input & Output)** âœ…
- **ASR Service**: Whisper model, ports 9000:9000, WHISPER_MODEL and WHISPER_DEVICE environment variables
- **TTS Service**: Kokoro/DIA 1.6B models, ports 9001:9001, TTS_MODEL and TTS_DEVICE environment variables

#### 3. **New Phase 4 Service (Vision Integration)** âœ…
- **Vision Service**: Qwen-VL 7B/MiniGPT-4 models, ports 9002:9002, VISION_MODEL and VISION_DEVICE environment variables

#### 4. **New Phase 6 Service (Internet LLM Integration)** âœ…
- **Internet LLM Service**: Llama 3/Mixtral models, ports 9003:9003, PERPLEXITY_API_KEY and OPENROUTER_API_KEY environment variables

#### 5. **New Phase 7-8 Services (Monitoring & Dashboard)** âœ…
- **Monitoring Service**: Prometheus and Grafana integration, ports 9004:9004, GPU_MONITORING and TOKEN_TRACKING enabled
- **Prometheus**: Metrics collection, ports 9090:9090, 200h retention time
- **Grafana**: Monitoring dashboards, ports 3000:3000, admin password configured

#### 6. **Infrastructure Services** âœ…
- **Redis**: Caching and session storage, ports 6379:6379
- **Nginx**: Reverse proxy for production, ports 80:80 and 443:443

#### 7. **Enhanced Volume Management** âœ…
- Added prometheus_data, grafana_data, redis_data volumes
- Updated existing volumes: postgres_data, ollama_data

### Implementation Status

#### âœ… **COMPLETED UPDATES:**
1. **README.md**: Complete overhaul with configurable LLM providers, new phases, enhanced architecture
2. **docker-compose.yml**: Comprehensive service architecture for all 10 phases
3. **DevLog.md**: Updated documentation of all changes and new features

#### ðŸ“‹ **New Phase Structure:**
- **Phase 1**: Text Chat + Hybrid Memory âœ… COMPLETE
- **Phase 2**: Voice Input & Output (ASR + TTS) ðŸš§
- **Phase 3**: Avatar Integration (PNG/Live2D/3D) ðŸš§
- **Phase 4**: Vision Integration (Webcam/Screen Share) ðŸš§
- **Phase 5**: Optional Memory LLM ðŸš§
- **Phase 6**: Internet LLM Integration (NEW) ðŸš§
- **Phase 7**: API Limit Monitoring & Token Management (NEW) ðŸš§
- **Phase 8**: Advanced Monitoring Dashboard (NEW) ðŸš§
- **Phase 9**: OBS + Twitch Chat Integration (renumbered) ðŸš§
- **Phase 10**: Advanced Discord Bot Features (renumbered) ðŸš§

### Key Features Added

1. **Configurable LLM Sources**: Each AI component can use different providers (Local/Ollama, API/OpenAI/Claude, Free/OpenRouter)
2. **API Limit Management**: Smart monitoring with "Luma getting sleepy/asleep" notifications
3. **GPU Monitoring**: Real-time tracking for Ollama users with Grafana dashboards
4. **Internet Integration**: Web browsing capabilities for current information retrieval
5. **Cost Optimization**: Automatic fallback to free/cheaper models when limits are reached
6. **Comprehensive Monitoring**: Token usage, GPU stats, service health in unified dashboard
7. **Production-Ready Architecture**: Nginx reverse proxy, Redis caching, Prometheus/Grafana monitoring

### Next Implementation Steps

1. **Phase 2**: Implement ASR/TTS services with Whisper and Kokoro/DIA models
2. **Phase 3**: Develop avatar rendering with Three.js/Pixi.js for lip-sync and expressions
3. **Phase 4**: Integrate vision processing with Qwen-VL/MiniGPT-4 models
4. **Phase 6**: Build internet search capabilities with web browsing LLMs
5. **Phase 7-8**: Deploy monitoring stack with Prometheus/Grafana and token tracking
6. **Phase 9**: Implement OBS browser source and Twitch chat integration
7. **Phase 10**: Enhance Discord bot with multi-speaker voice channels

The project now has a comprehensive blueprint supporting all 10 phases with full configurability, monitoring, and production-ready architecture.

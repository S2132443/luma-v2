# Multimodal AI VTuber/Streamer Implementation Plan

## Phase 1: Text Chat + Hybrid Memory + Web Interface

### Overview
Extend the current Luma-v2 Discord bot architecture to implement Phase 1 functionality: text chat with hybrid memory system, enhanced web interface with document upload capabilities, and configurable AI memory suggestions. This phase enhances the existing Flask-based dashboard and Discord bot to include manual memory management, AI-suggested memories using the existing DeepSeek API with support for local Ollama models, and a new web chat interface.

### Goals
- Enhance current web-based chat interface with memory management capabilities
- Implement hybrid memory system with manual and AI-suggested entries
- Maintain existing settings and logging functionality
- Support both DeepSeek API and Ollama local models
- Preserve current Flask-based frontend while adding memory features
- Add new web chat interface replacing the landing page
- Implement document upload functionality for multiple formats (PDF, Excel, TXT, JSON, CSV)
- Add configurable AI memory suggestions toggle (off by default)
- Implement message history persistence using browser storage

### Technical Architecture

#### Backend Enhancements
- **API Configuration**: Extend current settings to support both DeepSeek API and Ollama endpoints
- **Memory System**: Enhance the existing `Memory` model to support different types:
  - Manual memories (user-added)
  - AI-suggested memories (automatically generated by LLM)
- **LLM Integration**: Implement a pluggable model interface supporting both DeepSeek API and Ollama
- **Memory Suggestions**: Modify LLM responses to include memory suggestions based on conversation content

#### Database Extensions
- Extend existing `Memory` table with:
  - `source` column: 'manual' or 'ai_suggested'
  - `importance` column: score for memory relevance
  - `tags` column: JSON array for memory categorization
- Potentially add a `memory_suggestions` table to track AI-generated memory proposals

#### Frontend Enhancements (Flask Templates)
- **Memory Management Panel**: Add section to current dashboard for viewing and managing memories
- **Memory Suggestions Interface**: Display AI-suggested memories with approve/reject options
- **Enhanced Chat Interface**: Add in-memory conversation context to settings page
- **Memory Search**: Add capability to search and filter memories

### Implementation Tasks

#### 1. Model & Database Extensions
- [x] Extend `shared/models.py` Memory model with source, importance, and tags fields
- [x] Update SQLAlchemy models to support new memory attributes
- [x] Create migration scripts to add new columns to existing memory table (Database schema changes are applied automatically by SQLAlchemy)

#### 2. Model Abstraction Layer
- [x] Create `backend/llm_interface.py` for pluggable AI models
- [x] Implement DeepSeek adapter (extending current functionality)
- [x] Implement Ollama adapter
- [x] Create configuration system to switch between model providers

#### 3. Memory Management Backend
- [x] Create `backend/memory_manager.py` for enhanced memory operations
- [x] Implement memory suggestion logic in chat flow
- [x] Add API endpoints for memory search and management
- [x] Implement memory retrieval system to enhance context

#### 4. Web Dashboard Enhancements
- [x] Update `/memory` endpoint with enhanced memory management features
- [x] Add memory suggestion approval interface
- [x] Implement memory search and filtering
- [x] Add model provider selection in settings

#### 5. Discord Bot Integration
- [x] Enhance current `/chat` command to generate memory suggestions
- [x] Modify conversation context to include relevant memories
- [ ] Add new memory-related commands (optional)
- [x] Update token usage tracking to account for memory retrieval

#### 6. Configuration & Settings
- [x] Extend settings page to support Ollama endpoint configuration
- [x] Add model provider toggle (DeepSeek vs Ollama)
- [x] Implement model-specific settings (temperature, max_tokens, etc.)
- [x] Preserve backward compatibility with existing settings

#### 7. Testing & Validation
- [x] Write tests for new memory functionality
- [ ] Test both DeepSeek and Ollama integration
- [ ] Validate memory suggestion accuracy
- [ ] Performance testing with increasing memory size

#### 8. Documentation
- [x] Update README with Phase 1 features
- [x] Document new API endpoints
- [x] Add configuration examples for Ollama
- [x] Update Docker compose for potential Ollama service (optional)

### Implementation Flow

```
User Text → Model Selection (DeepSeek/Ollama) → Memory Retrieval → Main LLM →
          ├─ Text Response → Frontend/Discord
          └─ Memory Suggestion → Memory DB (if approved)
                        ▲
                        │
           User manual add/approve/delete/update
```

### Database Schema Changes
```sql
-- Extend existing memories table
ALTER TABLE memories ADD COLUMN source VARCHAR(20) DEFAULT 'manual';
ALTER TABLE memories ADD COLUMN importance INTEGER DEFAULT 0;
ALTER TABLE memories ADD COLUMN tags JSON;
ALTER TABLE memories ADD COLUMN approved BOOLEAN DEFAULT TRUE;
```

### New API Endpoints
- `GET /api/memory/search?q=query` - Search memories by content
- `POST /api/memory/suggest` - Approve AI-suggested memory
- `PUT /api/memory/{id}` - Update memory importance/tags
- `GET /api/models/available` - List available models/providers

### Configuration Settings
- `model_provider`: 'deepseek' or 'ollama'
- `deepseek_api_key`: API key for DeepSeek
- `ollama_endpoint`: URL for Ollama service (e.g., 'http://ollama:11434')
- `ollama_model`: Name of Ollama model to use
- `memory_context_limit`: Max memories to include in context
- `memory_suggestion_threshold`: Minimum importance score for suggestions

### Docker Compose Updates
```yaml
# Optional Ollama service (for local models)
ollama:
  image: ollama/ollama:latest
  ports:
    - "11434:11434"
  volumes:
    - ollama_data:/root/.ollama
  # Uncomment to run on GPU
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]

webapp:
  # Add environment for model selection
  environment:
    - MODEL_PROVIDER=deepseek  # or ollama
    - OLLAMA_ENDPOINT=http://ollama:11434  # if using ollama
```

### Success Criteria
- [x] Users can view, add, edit, and delete memories via web interface
- [x] AI suggests relevant memories during conversations
- [x] Both DeepSeek API and Ollama models work seamlessly
- [x] Memory retrieval improves conversation context
- [x] Existing functionality remains intact
- [x] Performance remains acceptable with growing memory size

### Timeline
- **Week 1**: Database extensions and model abstraction
- **Week 2**: Memory management backend and APIs
- **Week 3**: Frontend enhancements and Discord bot updates
- **Week 4**: Testing, documentation, and deployment

### Next Steps
After Phase 1 completion, prepare for Phase 2 by:
- Setting up ASR/TTS service infrastructure
- Planning WebSocket integration for real-time chat
- Preparing frontend for real-time updates

## Additional Features Beyond Original Scope

The following features were implemented beyond the original Phase 1 plan:

### Web Chat Interface
- Replaced landing page with full-featured chat interface
- Real-time messaging with history
- Default user ID for web-based interactions
- Responsive design compatible with existing dashboard

### Document Upload System
- Support for multiple file formats (PDF, Excel, TXT, JSON, CSV)
- Automatic content extraction and processing
- Integration with memory system to store document content
- File validation and error handling

### Memory Suggestions Control
- Configurable toggle in settings (off by default)
- Backend logic to conditionally generate suggestions
- Token usage optimization when disabled

### Message Persistence
- Browser-based chat history storage using localStorage
- Automatic loading of previous conversations
- Message history limits to prevent storage bloat

### Ollama Model Detection Feature
- Dynamic model detection from Ollama service
- API endpoint to fetch available models from Ollama
- JavaScript to populate dropdown with available models
- Multiple endpoint fallbacks to handle different network configurations (host.docker.internal, direct IP, etc.)

### ChatGPT-style File Upload
- Compact "Upload" button integrated with text input area
- Improved UI alignment with consistent element heights
- Better user experience with file selection and upload feedback

### Settings Page Enhancement
- Dynamic UI that shows appropriate fields based on model provider selection
- Proper JavaScript initialization to show correct section on page load
- Correct toggle behavior between DeepSeek and Ollama settings

## Phase 1: COMPLETE ✅

Phase 1 has been successfully implemented with all core features:
- Enhanced memory system with manual and AI-suggested entries
- Multi-model support (DeepSeek API and Ollama)
- Improved web dashboard with memory management
- Discord bot with memory suggestion capabilities
- API endpoints for memory operations
- Complete Docker configuration with optional Ollama service
- New web chat interface with document upload
- Configurable memory suggestions with toggle
- Message history persistence
- Ollama model detection and dynamic settings
- ChatGPT-style file upload interface
- Improved UI alignment and user experience